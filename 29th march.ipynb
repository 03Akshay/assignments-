{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e6570cb-e81a-41e7-a641-8fd9713c5262",
   "metadata": {},
   "source": [
    "## Q1. What is Lasso Regression, and how does it differ from other regression techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d337356a-4070-492f-84eb-ccdc4df771ab",
   "metadata": {},
   "source": [
    "##\n",
    "Lasso Regression, short for Least Absolute Shrinkage and Selection Operator, is a linear regression technique that incorporates L1 regularization. Lasso Regression is used for both feature selection and regularization to prevent overfitting. It differs from other regression techniques, such as ordinary least squares (OLS) regression, Ridge Regression (L2 regularization), and Elastic Net Regression (combination of L1 and L2 regularization), in the way it introduces sparsity in the model.\n",
    "\n",
    "The main features of Lasso Regression and its differences from other regression techniques are as follows:\n",
    "\n",
    "L1 Regularization (Lasso):\n",
    "In Lasso Regression, an L1 penalty term is added to the cost function. The L1 penalty is the sum of the absolute values of the coefficients multiplied by a regularization parameter (alpha or lambda). The L1 penalty tends to shrink some coefficients to exactly zero, effectively eliminating the corresponding features from the model. This property makes Lasso Regression useful for feature selection, as it can perform automatic feature selection by identifying irrelevant or less important features and excluding them from the model.\n",
    "\n",
    "Difference from OLS Regression:\n",
    "In OLS regression, there is no regularization, and all coefficients are estimated without any constraint. OLS regression can be sensitive to multicollinearity, leading to unstable coefficient estimates, especially when there are highly correlated predictor variables.\n",
    "\n",
    "Difference from Ridge Regression:\n",
    "In Ridge Regression, an L2 penalty term is added to the cost function. The L2 penalty is the sum of the squared values of the coefficients multiplied by a regularization parameter (alpha or lambda). Ridge Regression helps reduce the impact of multicollinearity and stabilizes the coefficient estimates but does not set any coefficient exactly to zero unless the regularization parameter becomes infinite.\n",
    "\n",
    "Difference from Elastic Net Regression:\n",
    "Elastic Net Regression combines both L1 (Lasso) and L2 (Ridge) regularization terms. It can be used to handle situations where there are many correlated features. While Elastic Net can perform feature selection like Lasso, it also allows for the simultaneous inclusion of groups of correlated features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da45e952-9c9e-4fb5-9429-772ee10c9124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [85.06299401 72.69174161]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate synthetic data with noise (linear relationship)\n",
    "X, y = make_regression(n_samples=100, n_features=2, noise=10, random_state=42)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the Lasso regression model with alpha (regularization strength) = 1.0\n",
    "lasso_model = Lasso(alpha=1.0)\n",
    "lasso_model.fit(X_train, y_train)\n",
    "\n",
    "# Get the coefficients of the Lasso Regression model\n",
    "coefficients = lasso_model.coef_\n",
    "\n",
    "print(\"Coefficients:\", coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08079bcb-a2a0-4689-938b-9aa61ce41421",
   "metadata": {},
   "source": [
    "## Q2. What is the main advantage of using Lasso Regression in feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693282e7-1ea2-43f9-8c26-fc5e3286f8fe",
   "metadata": {},
   "source": [
    "##\n",
    "The main advantage of using Lasso Regression in feature selection is its ability to automatically identify and select relevant features while setting the coefficients of irrelevant or less important features to exactly zero. This property of Lasso Regression allows for effective and automatic feature selection, which can be extremely valuable when dealing with high-dimensional datasets or when trying to identify the most influential predictors for a given target variable.\n",
    "\n",
    "The primary advantages of Lasso Regression for feature selection are:\n",
    "\n",
    "Sparsity and Dimensionality Reduction: Lasso Regression introduces sparsity by shrinking some coefficients to zero. As a result, it automatically selects a subset of features from the original set of predictors, effectively reducing the dimensionality of the problem. This can be beneficial when dealing with datasets with many predictors, as it simplifies the model and helps avoid overfitting.\n",
    "\n",
    "Feature Interpretability: Since Lasso Regression sets some coefficients to exactly zero, the resulting model includes only a subset of the original features. This leads to a more interpretable model with a clear understanding of the most influential predictors on the target variable.\n",
    "\n",
    "Improved Generalization: By automatically excluding irrelevant or redundant features, Lasso Regression helps in creating a more parsimonious model that is less prone to overfitting. The selected features are more likely to generalize well to new data, leading to better model performance.\n",
    "\n",
    "Addressing Multicollinearity: Lasso Regression can handle multicollinearity between predictors effectively by selecting only one of the correlated features and setting others to zero. This property makes it useful when dealing with highly correlated predictors.\n",
    "\n",
    "In contrast, other feature selection techniques may require manual tuning of thresholds or coefficients to select features, which can be time-consuming and may not guarantee the best subset of predictors. Lasso Regression's ability to perform automatic and data-driven feature selection makes it a powerful tool for handling high-dimensional data and improving the interpretability and generalization of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df346f33-2663-4e1f-9143-9359bb5c5803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features: [0 1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate synthetic data with noise (linear relationship)\n",
    "X, y = make_regression(n_samples=100, n_features=5, noise=10, random_state=42)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the Lasso regression model with alpha (regularization strength) = 1.0\n",
    "lasso_model = Lasso(alpha=1.0)\n",
    "lasso_model.fit(X_train, y_train)\n",
    "\n",
    "# Get the coefficients of the Lasso Regression model\n",
    "coefficients = lasso_model.coef_\n",
    "\n",
    "# Get the selected features (those with non-zero coefficients)\n",
    "selected_features = np.where(coefficients != 0)[0]\n",
    "\n",
    "print(\"Selected Features:\", selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d852e8d-8b30-4d88-8fab-fef96ff0eecb",
   "metadata": {},
   "source": [
    "## Q3. How do you interpret the coefficients of a Lasso Regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25396f2-5215-4edd-adbe-ff43bc0615f5",
   "metadata": {},
   "source": [
    "##\n",
    "Interpreting the coefficients of a Lasso Regression model is similar to interpreting the coefficients in ordinary least squares (OLS) regression. However, there is a key difference due to L1 regularization in Lasso Regression. The coefficients represent the strength and direction of the relationship between each independent variable and the dependent variable. Additionally, in Lasso Regression, some coefficients may be exactly zero, indicating that the corresponding features are excluded from the model.\n",
    "\n",
    "Interpreting the coefficients in Lasso Regression involves considering the following aspects:\n",
    "\n",
    "Magnitude of Coefficients: The magnitude of the coefficients represents the strength of the relationship between each independent variable and the dependent variable. Larger magnitude coefficients indicate a stronger influence on the target variable. However, it's important to note that Lasso Regression tends to shrink some coefficients to exactly zero, making them smaller than what you would obtain in OLS regression.\n",
    "\n",
    "Sign of Coefficients: The sign of the coefficients (positive or negative) indicates the direction of the relationship between each independent variable and the dependent variable. A positive coefficient suggests a positive correlation, meaning an increase in the independent variable tends to lead to an increase in the dependent variable. Conversely, a negative coefficient suggests a negative correlation, where an increase in the independent variable leads to a decrease in the dependent variable.\n",
    "\n",
    "Zero Coefficients: Lasso Regression has the unique property of setting some coefficients exactly to zero. This feature allows for automatic feature selection, where features with zero coefficients are considered irrelevant or less important for predicting the target variable. The selected features are those with non-zero coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3439e487-2f68-49f3-97b5-5b7b05e65808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [85.06299401 72.69174161]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate synthetic data with noise (linear relationship)\n",
    "X, y = make_regression(n_samples=100, n_features=2, noise=10, random_state=42)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the Lasso regression model with alpha (regularization strength) = 1.0\n",
    "lasso_model = Lasso(alpha=1.0)\n",
    "lasso_model.fit(X_train, y_train)\n",
    "\n",
    "# Get the coefficients of the Lasso Regression model\n",
    "coefficients = lasso_model.coef_\n",
    "\n",
    "print(\"Coefficients:\", coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5459d1b0-c3bb-4e7d-8d82-e7fb009ea5ff",
   "metadata": {},
   "source": [
    "## Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the model's performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65537cf-45be-4ac9-8303-3ae83bb3b9ba",
   "metadata": {},
   "source": [
    "In Lasso Regression, the main tuning parameter that can be adjusted is the regularization strength, often denoted as alpha or lambda. The regularization strength controls the trade-off between fitting the data (minimizing the sum of squared residuals) and introducing sparsity (setting some coefficients exactly to zero) to prevent overfitting.\n",
    "\n",
    "Regularization Strength (alpha or lambda):\n",
    "The regularization strength controls the amount of shrinkage applied to the coefficients. A larger value of alpha leads to stronger regularization and more aggressive shrinking of coefficients towards zero. Conversely, a smaller value of alpha reduces the regularization effect, allowing the coefficients to deviate more from zero and potentially leading to overfitting.\n",
    "When alpha is set to zero, Lasso Regression becomes equivalent to ordinary least squares (OLS) regression, as there is no regularization, and all coefficients are estimated without any constraint.\n",
    "Adjusting the regularization strength is crucial in Lasso Regression to find the optimal balance between fitting the data and controlling the model's complexity. This tuning parameter can be adjusted using cross-validation or other methods to select the best alpha value that maximizes model performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7c83af3-3e8a-47ac-a8d5-087492f52b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# Generate synthetic data with noise (linear relationship)\n",
    "X, y = make_regression(n_samples=100, n_features=5, noise=10, random_state=42)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create Lasso Regression model\n",
    "lasso_model = Lasso()\n",
    "\n",
    "# Set up a grid of possible alpha values to search through\n",
    "param_grid = {'alpha': np.logspace(-4, 0, 100)}\n",
    "\n",
    "# Perform Grid Search Cross-Validation to find the best alpha value\n",
    "grid_search = GridSearchCV(lasso_model, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best alpha value\n",
    "best_alpha = grid_search.best_params_['alpha']\n",
    "\n",
    "# Fit the Lasso model with the best alpha value\n",
    "best_lasso_model = Lasso(alpha=best_alpha)\n",
    "best_lasso_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = best_lasso_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfe56f5-6225-4916-98ff-5505625c9090",
   "metadata": {},
   "source": [
    "## Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6abcc28-bc62-4f73-9e37-816c3d27410b",
   "metadata": {},
   "source": [
    "##\n",
    "Lasso Regression is primarily designed for linear regression problems, where the relationship between the independent variables and the dependent variable is assumed to be linear. However, Lasso Regression can be extended to handle non-linear regression problems by incorporating polynomial features or other non-linear transformations of the original features.\n",
    "\n",
    "To use Lasso Regression for non-linear regression problems in Python, you can follow these steps:\n",
    "\n",
    "Create Non-Linear Features: If the relationship between the independent variables and the dependent variable is non-linear, you can create polynomial features or other non-linear transformations of the original features. For example, you can use PolynomialFeatures from scikit-learn to generate polynomial features.\n",
    "\n",
    "Fit Lasso Regression Model: After creating the non-linear features, you can fit the Lasso regression model using the extended feature matrix, which includes the original features and the non-linear transformations.\n",
    "\n",
    "Tune Regularization Strength: As in linear regression, you may need to tune the regularization strength (alpha) in Lasso Regression to find the best balance between fitting the data and controlling model complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5981125b-3e85-4831-99ea-212ffa4a1db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE (Lasso Regression with Polynomial Features): 2.2002769651039733\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generate synthetic non-linear data with noise\n",
    "np.random.seed(42)\n",
    "X = np.linspace(-2, 2, 100).reshape(-1, 1)\n",
    "y = 2 * X + X**3 + np.random.normal(0, 1, size=X.shape[0]).reshape(-1, 1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create polynomial features (non-linear transformation of X)\n",
    "degree = 3\n",
    "poly_features = PolynomialFeatures(degree=degree)\n",
    "X_train_poly = poly_features.fit_transform(X_train)\n",
    "X_test_poly = poly_features.transform(X_test)\n",
    "\n",
    "# Fit the Lasso regression model with alpha (regularization strength) = 1.0\n",
    "lasso_model = Lasso(alpha=1.0)\n",
    "lasso_model.fit(X_train_poly, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = lasso_model.predict(X_test_poly)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) to evaluate the model performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(\"MSE (Lasso Regression with Polynomial Features):\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0982083f-0e74-45ba-bcee-aee9722ef203",
   "metadata": {},
   "source": [
    "## Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd3a340-f6a1-47e9-b1e5-f7a47f3b2d75",
   "metadata": {},
   "source": [
    "##\n",
    "Lasso Regression is primarily designed for linear regression problems, where the relationship between the independent variables and the dependent variable is assumed to be linear. However, Lasso Regression can be extended to handle non-linear regression problems by incorporating polynomial features or other non-linear transformations of the original features.\n",
    "\n",
    "To use Lasso Regression for non-linear regression problems in Python, you can follow these steps:\n",
    "\n",
    "Create Non-Linear Features: If the relationship between the independent variables and the dependent variable is non-linear, you can create polynomial features or other non-linear transformations of the original features. For example, you can use PolynomialFeatures from scikit-learn to generate polynomial features.\n",
    "\n",
    "Fit Lasso Regression Model: After creating the non-linear features, you can fit the Lasso regression model using the extended feature matrix, which includes the original features and the non-linear transformations.\n",
    "\n",
    "Tune Regularization Strength: As in linear regression, you may need to tune the regularization strength (alpha) in Lasso Regression to find the best balance between fitting the data and controlling model complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79a41e10-56f2-41d1-8ad7-e38f9c1e01f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE (Lasso Regression with Polynomial Features): 2.2002769651039733\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generate synthetic non-linear data with noise\n",
    "np.random.seed(42)\n",
    "X = np.linspace(-2, 2, 100).reshape(-1, 1)\n",
    "y = 2 * X + X**3 + np.random.normal(0, 1, size=X.shape[0]).reshape(-1, 1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create polynomial features (non-linear transformation of X)\n",
    "degree = 3\n",
    "poly_features = PolynomialFeatures(degree=degree)\n",
    "X_train_poly = poly_features.fit_transform(X_train)\n",
    "X_test_poly = poly_features.transform(X_test)\n",
    "\n",
    "# Fit the Lasso regression model with alpha (regularization strength) = 1.0\n",
    "lasso_model = Lasso(alpha=1.0)\n",
    "lasso_model.fit(X_train_poly, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = lasso_model.predict(X_test_poly)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) to evaluate the model performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(\"MSE (Lasso Regression with Polynomial Features):\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335634f5-2171-4dba-86f8-5d79c09d0766",
   "metadata": {},
   "source": [
    "## Q6. What is the difference between Ridge Regression and Lasso Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c8698b-c303-4a98-b34f-57df8769e94b",
   "metadata": {},
   "source": [
    "##\n",
    "Ridge Regression and Lasso Regression are both linear regression techniques that introduce regularization to prevent overfitting and improve the model's generalization on unseen data. The key difference between Ridge Regression and Lasso Regression lies in the type of regularization they use and the impact on the model's coefficients.\n",
    "\n",
    "Regularization Type:\n",
    "\n",
    "Ridge Regression: Ridge Regression uses L2 regularization, where the regularization term is the sum of the squared values of the coefficients multiplied by a regularization parameter (alpha or lambda). The L2 regularization term is added to the cost function, penalizing large coefficients and reducing their magnitude, but never setting them exactly to zero.\n",
    "\n",
    "Lasso Regression: Lasso Regression uses L1 regularization, where the regularization term is the sum of the absolute values of the coefficients multiplied by a regularization parameter (alpha or lambda). The L1 regularization term is added to the cost function, penalizing large coefficients and driving some of them exactly to zero. This leads to automatic feature selection, where some predictors are excluded from the model.\n",
    "\n",
    "Coefficient Shrinkage:\n",
    "\n",
    "Ridge Regression: Ridge Regression tends to shrink the coefficients towards zero without eliminating them entirely. As the regularization parameter increases, the coefficient values decrease, but they are unlikely to be exactly zero, even for very high regularization values.\n",
    "\n",
    "Lasso Regression: Lasso Regression performs both regularization and feature selection. It tends to shrink some coefficients to exactly zero, effectively excluding the corresponding features from the model. This makes Lasso Regression useful for feature selection, as it can identify and exclude irrelevant or less important predictors.\n",
    "\n",
    "Number of Selected Features:\n",
    "\n",
    "Ridge Regression: Ridge Regression does not perform feature selection. It keeps all features in the model, although their coefficients are reduced due to regularization. This can be beneficial when dealing with a large number of predictors that are potentially relevant to the target variable.\n",
    "Lasso Regression: Lasso Regression performs automatic feature selection by setting some coefficients to exactly zero. The selected features are the ones with non-zero coefficients, and the eliminated features are considered irrelevant or less important.\n",
    "\n",
    "In summary, the main difference between Ridge Regression and Lasso Regression lies in the type of regularization and its effect on the model's coefficients. Ridge Regression can shrink coefficients but does not exclude them, while Lasso Regression can eliminate some coefficients altogether, leading to a more interpretable and sparse model. The choice between Ridge and Lasso Regression (or even Elastic Net, which combines both types of regularization) depends on the specific problem, the nature of the data, and the desired characteristics of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e3cd38-c08f-4a4c-a60f-15917002b988",
   "metadata": {},
   "source": [
    "## Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbfed0b-e508-4d5e-942d-f628371fa30e",
   "metadata": {},
   "source": [
    "##\n",
    "Yes, Lasso Regression can handle multicollinearity in the input features to some extent. Multicollinearity occurs when two or more independent variables are highly correlated, which can lead to unstable and unreliable coefficient estimates in linear regression models.\n",
    "\n",
    "In Lasso Regression, the L1 regularization (absolute value penalty) can help address multicollinearity by encouraging the model to select only one of the correlated features and set the coefficients of the others to exactly zero. This sparsity-inducing property of Lasso Regression can be beneficial in situations where there are highly correlated predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f20a805-b763-48a7-b33c-a476bee2e0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [63.87749536 23.5618251 ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Generate synthetic data with multicollinearity\n",
    "np.random.seed(42)\n",
    "X, y = make_regression(n_samples=100, n_features=2, noise=10, random_state=42)\n",
    "\n",
    "# Introduce multicollinearity by making the second feature highly correlated with the first one\n",
    "X[:, 1] = X[:, 0] + np.random.normal(0, 0.1, size=X.shape[0])\n",
    "\n",
    "# Fit the Lasso regression model with alpha (regularization strength) = 1.0\n",
    "lasso_model = Lasso(alpha=1.0)\n",
    "lasso_model.fit(X, y)\n",
    "\n",
    "# Get the coefficients of the Lasso Regression model\n",
    "coefficients = lasso_model.coef_\n",
    "\n",
    "print(\"Coefficients:\", coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e6509f-a091-4e86-81a6-d9e70069c307",
   "metadata": {},
   "source": [
    "## Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d386e462-2101-471b-a275-ceed935a0770",
   "metadata": {},
   "source": [
    "##\n",
    "Selecting the optimal value of the regularization parameter (lambda or alpha) in Lasso Regression is a critical step to ensure the model's best performance and balance between bias and variance. One common approach to choose the optimal value is through cross-validation, where the dataset is split into training and validation sets to evaluate the model's performance for different alpha values. The value of alpha that results in the best performance on the validation set is considered the optimal regularization parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e2762e8-203c-4f6b-8442-608afb2ef5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Alpha: 0.08111308307896872\n",
      "MSE (Lasso Regression with Best Alpha): 2799.8279461101843\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the diabetes dataset (as an example)\n",
    "diabetes = load_diabetes()\n",
    "X, y = diabetes.data, diabetes.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define a range of alpha values to search through\n",
    "alphas = np.logspace(-4, 0, 100)\n",
    "\n",
    "# Create a Lasso Regression model\n",
    "lasso_model = Lasso()\n",
    "\n",
    "# Perform Grid Search Cross-Validation to find the best alpha value\n",
    "grid_search = GridSearchCV(lasso_model, param_grid={'alpha': alphas}, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best alpha value\n",
    "best_alpha = grid_search.best_params_['alpha']\n",
    "\n",
    "# Fit the Lasso model with the best alpha value\n",
    "best_lasso_model = Lasso(alpha=best_alpha)\n",
    "best_lasso_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = best_lasso_model.predict(X_test)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) to evaluate the model performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(\"Best Alpha:\", best_alpha)\n",
    "print(\"MSE (Lasso Regression with Best Alpha):\", mse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
