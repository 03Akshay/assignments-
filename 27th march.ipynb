{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b17d5d0-c55e-4577-afe4-932e09685e2b",
   "metadata": {},
   "source": [
    "## Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebfb460-2bbf-40f7-b333-446da60f515b",
   "metadata": {},
   "source": [
    "##\n",
    "R-squared (R²), also known as the coefficient of determination, is a statistical measure used to evaluate the goodness of fit of a linear regression model. It represents the proportion of the variance in the dependent variable (Y) that is explained by the independent variables (X) in the model. In other words, R-squared quantifies how well the independent variables explain the variation in the dependent variable.\n",
    "\n",
    "R-squared ranges from 0 to 1, where:\n",
    "\n",
    "R² = 0 indicates that the independent variables do not explain any of the variation in the dependent variable. The model does not fit the data well.\n",
    "\n",
    "R² = 1 indicates that the independent variables perfectly explain all the variation in the dependent variable. The model fits the data perfectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7df1558-0dc1-4040-a54c-1d65684a2bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared: 0.6000000000000001\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Sample data for linear regression\n",
    "X = np.array([1, 2, 3, 4, 5])\n",
    "Y = np.array([2, 4, 5, 4, 5])\n",
    "\n",
    "# Reshape X to a 2D array\n",
    "X = X.reshape(-1, 1)\n",
    "\n",
    "# Create the linear regression model\n",
    "linear_model = LinearRegression()\n",
    "\n",
    "# Fit the model on the data\n",
    "linear_model.fit(X, Y)\n",
    "\n",
    "# Get the R-squared value\n",
    "r_squared = linear_model.score(X, Y)\n",
    "\n",
    "print(\"R-squared:\", r_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4629ef3b-a32f-430e-8b0a-95a8c14d2877",
   "metadata": {},
   "source": [
    "## Q2. Define adjusted R-squared and explain how it differs from the regular R-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1893104f-5887-4b6b-8311-9e09a48d731a",
   "metadata": {},
   "source": [
    "## \n",
    "Adjusted R-squared is a modified version of the regular R-squared (coefficient of determination) that takes into account the number of independent variables in the linear regression model. It addresses a potential issue with the regular R-squared, where adding more independent variables to the model can artificially increase the R-squared value, even if the added variables do not significantly improve the model's performance.\n",
    "\n",
    "While the regular R-squared (R²) measures the proportion of the variance in the dependent variable explained by the independent variables, the adjusted R-squared (R²_adj) penalizes the addition of independent variables that do not contribute significantly to the model's explanatory power. It helps to determine whether additional independent variables improve the model's fit beyond what would be expected by chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1636b5b4-4b0e-462b-89d1-32cab715013b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared: 0.6000000000000001\n",
      "Adjusted R-squared: 0.4666666666666668\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Sample data for linear regression\n",
    "X = np.array([1, 2, 3, 4, 5])\n",
    "Y = np.array([2, 4, 5, 4, 5])\n",
    "\n",
    "# Reshape X to a 2D array\n",
    "X = X.reshape(-1, 1)\n",
    "\n",
    "# Create the linear regression model\n",
    "linear_model = LinearRegression()\n",
    "\n",
    "# Fit the model on the data\n",
    "linear_model.fit(X, Y)\n",
    "\n",
    "# Get the R-squared value\n",
    "r_squared = linear_model.score(X, Y)\n",
    "\n",
    "# Calculate the adjusted R-squared value\n",
    "n = len(Y)\n",
    "p = X.shape[1]  # Number of independent variables (features)\n",
    "r_squared_adj = 1 - (1 - r_squared) * (n - 1) / (n - p - 1)\n",
    "\n",
    "print(\"R-squared:\", r_squared)\n",
    "print(\"Adjusted R-squared:\", r_squared_adj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5172b5b2-23e5-4a17-8283-e722dca225b2",
   "metadata": {},
   "source": [
    "## Q3. When is it more appropriate to use adjusted R-squared?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c36757-ccaa-4109-9b18-58fd2b714e6e",
   "metadata": {},
   "source": [
    "## \n",
    "\n",
    "The adjusted R-squared (R²_adj) is more appropriate to use when you want to evaluate the goodness of fit of a linear regression model that includes multiple independent variables. It addresses the concern of the regular R-squared (R²) artificially increasing with the addition of independent variables, regardless of their significance in improving the model's performance. The adjusted R-squared penalizes the inclusion of irrelevant variables, providing a more accurate evaluation of how well the independent variables explain the variation in the dependent variable.\n",
    "\n",
    "In general, if you are comparing different linear regression models with varying numbers of independent variables, using the adjusted R-squared is preferred over the regular R-squared. It helps to choose the most parsimonious model (the one with fewer irrelevant variables) while still providing a measure of how well the chosen independent variables explain the variability in the dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f7b601e-8f92-4540-8cbb-6e0fe569a763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared: 0.9941176470588236\n",
      "Adjusted R-squared: 0.9882352941176471\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Sample data for multiple linear regression\n",
    "X1 = np.array([1, 2, 3, 4, 5])\n",
    "X2 = np.array([2, 4, 5, 3, 6])\n",
    "Y = np.array([10, 20, 30, 25, 40])\n",
    "\n",
    "# Create a DataFrame with the independent variables\n",
    "data = pd.DataFrame({'X1': X1, 'X2': X2})\n",
    "\n",
    "# Create the linear regression model\n",
    "linear_model = LinearRegression()\n",
    "\n",
    "# Fit the model on the data\n",
    "linear_model.fit(data, Y)\n",
    "\n",
    "# Get the R-squared value and the number of independent variables\n",
    "r_squared = linear_model.score(data, Y)\n",
    "num_independent_variables = data.shape[1]\n",
    "\n",
    "# Calculate the adjusted R-squared value\n",
    "n = len(Y)\n",
    "r_squared_adj = 1 - (1 - r_squared) * (n - 1) / (n - num_independent_variables - 1)\n",
    "\n",
    "print(\"R-squared:\", r_squared)\n",
    "print(\"Adjusted R-squared:\", r_squared_adj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968103c4-0869-4a2b-a176-575f4fc7949c",
   "metadata": {},
   "source": [
    "## Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics calculated, and what do they represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f28db7e-67c7-4ec0-a591-5eb0fa542c18",
   "metadata": {},
   "source": [
    "## \n",
    "RMSE (Root Mean Squared Error), MSE (Mean Squared Error), and MAE (Mean Absolute Error) are common evaluation metrics used in the context of regression analysis to assess the performance of predictive models.\n",
    "\n",
    "Mean Squared Error (MSE):\n",
    "MSE measures the average of the squared differences between the predicted values and the actual values. It penalizes large errors more heavily than small errors, making it sensitive to outliers. A lower MSE indicates a better fit of the model to the data.\n",
    "\n",
    "The formula to calculate MSE is as follows:\n",
    "\n",
    "MSE = (1/n) * Σ(y_actual - y_predicted)^2\n",
    "\n",
    "Root Mean Squared Error (RMSE):\n",
    "RMSE is the square root of the MSE, and it represents the average magnitude of the error between the predicted values and the actual values. RMSE is commonly used because it is in the same unit as the dependent variable (Y), which makes it easier to interpret. Like MSE, a lower RMSE indicates a better fit of the model to the data.\n",
    "\n",
    "The formula to calculate RMSE is as follows:\n",
    "\n",
    "RMSE = √(MSE)\n",
    "\n",
    "Mean Absolute Error (MAE):\n",
    "MAE measures the average of the absolute differences between the predicted values and the actual values. Unlike MSE, MAE does not square the errors, making it less sensitive to outliers. A lower MAE also indicates a better fit of the model to the data.\n",
    "\n",
    "The formula to calculate MAE is as follows:\n",
    "\n",
    "MAE = (1/n) * Σ|y_actual - y_predicted|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3a84ea0-4d2b-4f43-acac-2eb74e6fe5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 5.0\n",
      "Root Mean Squared Error (RMSE): 2.23606797749979\n",
      "Mean Absolute Error (MAE): 2.2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Sample data for regression analysis\n",
    "y_actual = np.array([10, 20, 30, 40, 50])\n",
    "y_predicted = np.array([12, 18, 33, 38, 52])\n",
    "\n",
    "# Calculate Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_actual, y_predicted)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# Calculate Mean Absolute Error (MAE)\n",
    "mae = mean_absolute_error(y_actual, y_predicted)\n",
    "\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a7b12a-0185-4bb2-97da-9e24f6e86f19",
   "metadata": {},
   "source": [
    "## Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e51d01-37bd-4c93-a844-87af0fe46f26",
   "metadata": {},
   "source": [
    "##\n",
    "Advantages of Using RMSE, MSE, and MAE as Evaluation Metrics in Regression Analysis:\n",
    "\n",
    "Interpretability: RMSE, MSE, and MAE are easy to interpret, as they represent the average magnitude of errors between the predicted and actual values. They are in the same unit as the dependent variable (Y), making it easier to communicate the model's performance.\n",
    "\n",
    "Sensitivity to Errors: RMSE and MSE penalize large errors more heavily than small errors, which can be beneficial in cases where large errors are more critical to avoid.\n",
    "\n",
    "Commonly Used: RMSE, MSE, and MAE are widely used and well-understood metrics in regression analysis, making them a standard choice for model evaluation.\n",
    "\n",
    "Differentiation of Models: These metrics allow for a quantitative comparison of different regression models. Models with lower RMSE, MSE, or MAE generally perform better in approximating the actual values.\n",
    "\n",
    "Disadvantages of Using RMSE, MSE, and MAE as Evaluation Metrics in Regression Analysis:\n",
    "\n",
    "Sensitivity to Outliers: RMSE and MSE are sensitive to outliers, as they square the errors. Outliers can have a significant impact on these metrics, leading to potentially misleading evaluations.\n",
    "\n",
    "MAE versus MSE/RMSE: MAE does not differentiate between large and small errors, which might be desirable in certain applications. However, it can be less sensitive to model improvements compared to MSE and RMSE.\n",
    "\n",
    "Unit Dependence: MSE and RMSE have the disadvantage of being dependent on the scale of the dependent variable. This can make comparisons challenging when working with datasets with different scales.\n",
    "\n",
    "Non-Uniqueness: Different models can have the same RMSE, MSE, or MAE values, making it difficult to determine the uniqueness of the model's fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e01fbc09-550e-40f6-b9f8-33e2a96bf8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 5.0\n",
      "Root Mean Squared Error (RMSE): 2.23606797749979\n",
      "Mean Absolute Error (MAE): 2.2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Sample data for regression analysis\n",
    "y_actual = np.array([10, 20, 30, 40, 50])\n",
    "y_predicted = np.array([12, 18, 33, 38, 52])\n",
    "\n",
    "# Calculate Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_actual, y_predicted)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# Calculate Mean Absolute Error (MAE)\n",
    "mae = mean_absolute_error(y_actual, y_predicted)\n",
    "\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca771a67-e60f-43ba-b023-8be78eac93a9",
   "metadata": {},
   "source": [
    "## Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is it more appropriate to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dceddd-b594-4850-893a-4143f20879be",
   "metadata": {},
   "source": [
    "##\n",
    "Lasso (Least Absolute Shrinkage and Selection Operator) and Ridge are both regularization techniques used in linear regression to prevent overfitting and improve model performance. They differ in the penalty terms they add to the cost function during training.\n",
    "\n",
    "Lasso Regularization:\n",
    "\n",
    "Lasso regularization adds the L1 norm (absolute values) of the coefficients as a penalty term to the cost function. The L1 penalty encourages sparsity in the model by forcing some coefficients to be exactly zero, effectively performing feature selection. It can be useful when dealing with high-dimensional datasets, as it helps in automatically selecting the most relevant features and discarding irrelevant ones.\n",
    "\n",
    "The Lasso cost function is as follows:\n",
    "\n",
    "Cost = Sum of Squared Errors + alpha * Sum of Absolute Values of Coefficients\n",
    "\n",
    "Where:\n",
    "\n",
    "The first part is the Sum of Squared Errors, which is the same as the ordinary least squares (OLS) cost function in linear regression.\n",
    "The second part is the L1 penalty term, where alpha is the regularization parameter that controls the strength of regularization.\n",
    "Ridge Regularization:\n",
    "\n",
    "Ridge regularization adds the L2 norm (squared values) of the coefficients as a penalty term to the cost function. The L2 penalty encourages the coefficients to be close to zero but does not force them to be exactly zero. This helps in reducing the impact of multicollinearity in the data and can be useful when dealing with features that are highly correlated.\n",
    "\n",
    "The Ridge cost function is as follows:\n",
    "\n",
    "Cost = Sum of Squared Errors + alpha * Sum of Squared Values of Coefficients\n",
    "\n",
    "Where:\n",
    "\n",
    "The first part is the Sum of Squared Errors, the same as in OLS.\n",
    "The second part is the L2 penalty term, where alpha is the regularization parameter.\n",
    "When to Use Lasso or Ridge:\n",
    "\n",
    "Use Lasso when you suspect that many of the features are irrelevant or redundant. Lasso can perform feature selection by setting some coefficients to zero, effectively reducing the number of features in the model.\n",
    "\n",
    "Use Ridge when you have multicollinearity (high correlation) between the features. Ridge regularization can help stabilize the model and reduce the impact of multicollinearity by keeping the coefficients small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6f3d170-4ab3-4e8c-9649-e5b7f2ed62ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 2798.193485169719\n",
      "Model coefficients (excluding intercept): [   0.         -152.66477923  552.69777529  303.36515791  -81.36500664\n",
      "   -0.         -229.25577639    0.          447.91952518   29.64261704]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the diabetes dataset (a built-in dataset in scikit-learn)\n",
    "data = load_diabetes()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the Lasso regression model with alpha (regularization strength) = 0.1\n",
    "lasso_model = Lasso(alpha=0.1)\n",
    "\n",
    "# Fit the model on the training data\n",
    "lasso_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = lasso_model.predict(X_test)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) to evaluate the model performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"Model coefficients (excluding intercept):\", lasso_model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80aa5306-9905-4bd8-9ca7-9129371a5842",
   "metadata": {},
   "source": [
    "## Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an example to illustrate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e726e684-1397-4144-9228-5374db4da46c",
   "metadata": {},
   "source": [
    "##\n",
    "Regularized linear models help prevent overfitting in machine learning by adding a penalty term to the cost function during training. This penalty term discourages the model from relying too heavily on any particular feature or from fitting noise in the training data. Regularization introduces a bias towards simpler models with smaller coefficients, which can help in reducing model complexity and generalizing better to unseen data.\n",
    "\n",
    "In the case of linear regression, Lasso (L1 regularization) and Ridge (L2 regularization) are common regularization techniques.\n",
    "\n",
    "Lasso Regularization (L1):\n",
    "Lasso adds the absolute values of the coefficients as a penalty term to the cost function. This penalty encourages sparsity, meaning it tends to force some coefficients to exactly zero. As a result, Lasso can effectively perform feature selection by discarding less important features.\n",
    "\n",
    "Ridge Regularization (L2):\n",
    "Ridge adds the squared values of the coefficients as a penalty term to the cost function. This penalty discourages large coefficients and makes the model more stable. It can help in situations where the features are highly correlated, as Ridge regularization reduces the impact of multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd54ca8b-bff8-4add-b52d-c14cae44c447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE (Linear Regression): 0.006536995137169996\n",
      "MSE (Lasso Regression): 0.13798968398000147\n",
      "MSE (Ridge Regression): 0.0065035939946182396\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Sample data with noise (linear relationship)\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1)\n",
    "y = 3 * X.squeeze() + 2 + 0.1 * np.random.randn(100)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit a linear regression model without regularization (Ordinary Least Squares)\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train, y_train)\n",
    "\n",
    "# Fit a Lasso regression model (L1 regularization)\n",
    "lasso_model = Lasso(alpha=0.1)  # alpha is the regularization strength\n",
    "lasso_model.fit(X_train, y_train)\n",
    "\n",
    "# Fit a Ridge regression model (L2 regularization)\n",
    "ridge_model = Ridge(alpha=0.1)  # alpha is the regularization strength\n",
    "ridge_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_linear = linear_model.predict(X_test)\n",
    "y_pred_lasso = lasso_model.predict(X_test)\n",
    "y_pred_ridge = ridge_model.predict(X_test)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) to evaluate the models' performance\n",
    "mse_linear = mean_squared_error(y_test, y_pred_linear)\n",
    "mse_lasso = mean_squared_error(y_test, y_pred_lasso)\n",
    "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
    "\n",
    "print(\"MSE (Linear Regression):\", mse_linear)\n",
    "print(\"MSE (Lasso Regression):\", mse_lasso)\n",
    "print(\"MSE (Ridge Regression):\", mse_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697efba6-28e2-497f-b64d-b73216c4c79a",
   "metadata": {},
   "source": [
    "## Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best choice for regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7847aba-eff7-401c-a0a6-60c6b4fb16b0",
   "metadata": {},
   "source": [
    "##\n",
    "While regularized linear models like Lasso (L1 regularization) and Ridge (L2 regularization) offer significant advantages in preventing overfitting and handling multicollinearity, they do have some limitations and may not always be the best choice for regression analysis:\n",
    "\n",
    "1. Feature Selection Limitations:\n",
    "Regularized linear models, especially Lasso, can perform feature selection by setting some coefficients to exactly zero. However, this may not always be desirable, as some features may still contain valuable information even if their coefficients are small. Lasso tends to be more suitable when you suspect that many features are irrelevant or redundant.\n",
    "\n",
    "2. Hyperparameter Selection:\n",
    "Regularized linear models require tuning of the regularization strength (alpha parameter). Selecting the appropriate value of alpha can be challenging and may depend on the specific dataset. Poorly chosen alpha values can lead to suboptimal model performance.\n",
    "\n",
    "3. Noisy Data:\n",
    "If the dataset contains a significant amount of noise, regularized linear models may still overfit or underfit. In such cases, it's essential to preprocess the data and remove noise or outliers before applying regularization.\n",
    "\n",
    "4. Complex Relationships:\n",
    "Regularized linear models assume linear relationships between the features and the target variable. If the underlying relationship is highly nonlinear, regularized linear models may not capture the complexity of the data well.\n",
    "\n",
    "5. Computationally Expensive:\n",
    "Regularization adds an additional computational cost during model training compared to simple linear regression. As the dataset size increases, training regularized linear models can become computationally expensive.\n",
    "\n",
    "6. Non-Uniqueness:\n",
    "Different regularization methods or hyperparameters can lead to different models with similar performance. The choice of regularization may not always be unique, making it difficult to interpret the exact reasons behind model behavior.\n",
    "\n",
    "7. Collinear Features:\n",
    "Although Ridge regularization can help mitigate multicollinearity to some extent, it may not fully address the issue in datasets with highly correlated features. In such cases, feature engineering or other techniques may be required to deal with collinearity effectively.\n",
    "\n",
    "It's important to consider the specific characteristics of the dataset and the objectives of the regression analysis when deciding whether to use regularized linear models or other regression techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6646f9c4-c9e2-4ca0-9dd0-f0cf879e957d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE (Lasso Regression): 28.43115771757386\n",
      "MSE (Ridge Regression): 34.47403081579157\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generate synthetic data with complex relationships\n",
    "X, y = make_regression(n_samples=100, n_features=5, noise=5, random_state=42)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit a Lasso regression model (L1 regularization)\n",
    "lasso_model = Lasso(alpha=0.1)  # alpha is the regularization strength\n",
    "lasso_model.fit(X_train, y_train)\n",
    "y_pred_lasso = lasso_model.predict(X_test)\n",
    "mse_lasso = mean_squared_error(y_test, y_pred_lasso)\n",
    "\n",
    "# Fit a Ridge regression model (L2 regularization)\n",
    "ridge_model = Ridge(alpha=1.0)  # alpha is the regularization strength\n",
    "ridge_model.fit(X_train, y_train)\n",
    "y_pred_ridge = ridge_model.predict(X_test)\n",
    "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
    "\n",
    "print(\"MSE (Lasso Regression):\", mse_lasso)\n",
    "print(\"MSE (Ridge Regression):\", mse_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb923389-96e6-4e37-9a67-6565adad8b2b",
   "metadata": {},
   "source": [
    "## Q9. You are comparing the performance of two regression models using different evaluation metrics. Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better performer, and why? Are there any limitations to your choice of metric?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bc4abc-19a6-42ed-9ac9-cbb7388610bb",
   "metadata": {},
   "source": [
    "##\n",
    "When comparing the performance of two regression models, a lower value for evaluation metrics indicates better model performance. In this case, Model B has a lower Mean Absolute Error (MAE) of 8, which suggests that it performs better on average compared to Model A, which has a Root Mean Squared Error (RMSE) of 10.\n",
    "\n",
    "Model B would be the better performer based on the MAE metric. MAE represents the average absolute difference between the predicted values and the actual values. A lower MAE indicates that, on average, the model's predictions are closer to the actual values, making it a better choice when the goal is to minimize the average prediction error.\n",
    "\n",
    "However, it's essential to consider the context and specific characteristics of the problem. Different evaluation metrics may be more appropriate depending on the application and the relative importance of prediction errors. For example, RMSE might be more appropriate if larger errors should be penalized more heavily.\n",
    "\n",
    "Limitations of the Choice of Metric:\n",
    "\n",
    "Scale of the Dependent Variable: RMSE and MAE are in the same unit as the dependent variable (Y), but they may not be directly comparable if the scale of Y is significantly different between the models. In such cases, it's essential to scale the metrics or use relative metrics like Mean Absolute Percentage Error (MAPE).\n",
    "\n",
    "Sensitivity to Outliers: RMSE squares the errors, making it sensitive to large errors or outliers. If the dataset contains outliers, RMSE may be inflated, and MAE might be a more robust choice.\n",
    "\n",
    "Asymmetry in Errors: If the errors have an asymmetric impact on the model's performance (e.g., overestimation is more critical than underestimation), other metrics like Mean Bias Deviation (MBD) or Theil's U statistic may be more appropriate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68bac108-fac3-46c0-b2b7-d3b08ed57c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Model A): 2.23606797749979\n",
      "MAE (Model B): 1.4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Sample data for Model A and Model B\n",
    "y_actual = np.array([10, 20, 30, 40, 50])\n",
    "y_pred_model_a = np.array([12, 18, 33, 38, 52])\n",
    "y_pred_model_b = np.array([11, 22, 29, 41, 48])\n",
    "\n",
    "# Calculate RMSE for Model A\n",
    "rmse_model_a = np.sqrt(mean_squared_error(y_actual, y_pred_model_a))\n",
    "\n",
    "# Calculate MAE for Model B\n",
    "mae_model_b = mean_absolute_error(y_actual, y_pred_model_b)\n",
    "\n",
    "print(\"RMSE (Model A):\", rmse_model_a)\n",
    "print(\"MAE (Model B):\", mae_model_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1753263-ea97-4d07-b556-d043cba0c158",
   "metadata": {},
   "source": [
    "## Q10. You are comparing the performance of two regularized linear models using different types of regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the better performer, and why? Are there any trade-offs or limitations to your choice of regularization method?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee356856-0855-4797-8803-ebc3edce33fa",
   "metadata": {},
   "source": [
    "##\n",
    "To compare the performance of two regularized linear models using different types of regularization (Ridge and Lasso), we need to evaluate their performance on a specific metric (e.g., Mean Squared Error or Mean Absolute Error). The model with lower error on the test data is considered the better performer.\n",
    "\n",
    "However, the choice of regularization method and its corresponding regularization parameter (alpha) depends on the specific dataset and the characteristics of the problem. Both Ridge and Lasso regularization have their trade-offs and limitations, and the appropriate choice depends on the underlying data and the objective of the analysis.\n",
    "\n",
    "Ridge Regularization:\n",
    "\n",
    "Ridge regularization (L2 regularization) adds the squared values of the coefficients as a penalty term to the cost function. It helps in reducing the impact of multicollinearity and can be effective when dealing with highly correlated features. Ridge does not set any coefficients to exactly zero, allowing all features to contribute to the predictions.\n",
    "\n",
    "Lasso Regularization:\n",
    "\n",
    "Lasso regularization (L1 regularization) adds the absolute values of the coefficients as a penalty term to the cost function. It can effectively perform feature selection by setting some coefficients to exactly zero, resulting in a sparse model with fewer features. Lasso can be useful when dealing with high-dimensional datasets and identifying the most relevant features.\n",
    "\n",
    "Trade-offs and Limitations:\n",
    "\n",
    "Feature Selection: Lasso regularization can be more suitable when feature selection is important, as it tends to set some coefficients to zero, effectively performing feature selection. Ridge regularization does not perform feature selection and keeps all features in the model.\n",
    "\n",
    "Multicollinearity: Ridge regularization is more effective at handling multicollinearity, as it reduces the impact of correlated features by encouraging smaller but non-zero coefficients. Lasso can struggle with highly correlated features, and it might select only one feature out of a group of highly correlated features.\n",
    "\n",
    "Interpretability: Ridge regularization may lead to models with small but non-zero coefficients for all features, which can be more interpretable than Lasso models that select only a subset of features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27eb3e67-57d6-464c-aa2f-32ab77390049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE (Ridge Regression): 28.616343462151434\n",
      "MSE (Lasso Regression): 29.746439932475774\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generate synthetic data with noise (linear relationship)\n",
    "X, y = make_regression(n_samples=100, n_features=5, noise=5, random_state=42)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit a Ridge regression model (L2 regularization)\n",
    "ridge_model = Ridge(alpha=0.1)  # alpha is the regularization strength\n",
    "ridge_model.fit(X_train, y_train)\n",
    "y_pred_ridge = ridge_model.predict(X_test)\n",
    "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
    "\n",
    "# Fit a Lasso regression model (L1 regularization)\n",
    "lasso_model = Lasso(alpha=0.5)  # alpha is the regularization strength\n",
    "lasso_model.fit(X_train, y_train)\n",
    "y_pred_lasso = lasso_model.predict(X_test)\n",
    "mse_lasso = mean_squared_error(y_test, y_pred_lasso)\n",
    "\n",
    "print(\"MSE (Ridge Regression):\", mse_ridge)\n",
    "print(\"MSE (Lasso Regression):\", mse_lasso)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
